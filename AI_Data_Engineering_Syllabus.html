<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>AI-DE 4900: AI Data Engineering & Automation â€” Syllabus</title>
<style>
  @media print {
    body { font-size: 11pt; }
    .page-break { page-break-before: always; }
    h1, h2, h3 { page-break-after: avoid; }
    table { page-break-inside: avoid; }
    .project-box { page-break-inside: avoid; }
    @page { margin: 0.75in; }
  }

  * { box-sizing: border-box; margin: 0; padding: 0; }

  body {
    font-family: 'Georgia', 'Times New Roman', serif;
    line-height: 1.6;
    color: #1a1a1a;
    max-width: 8.5in;
    margin: 0 auto;
    padding: 0.75in;
    background: #fff;
  }

  /* Header / Title Block */
  .title-block {
    text-align: center;
    border-bottom: 3px double #2c3e50;
    padding-bottom: 20px;
    margin-bottom: 30px;
  }
  .title-block h1 {
    font-size: 24pt;
    font-weight: bold;
    color: #2c3e50;
    margin-bottom: 4px;
    letter-spacing: 0.5px;
  }
  .title-block .subtitle {
    font-size: 14pt;
    font-style: italic;
    color: #555;
    margin-bottom: 12px;
  }
  .title-block .program-line {
    font-size: 11pt;
    color: #333;
  }

  /* Course Info Table */
  .info-table {
    width: 100%;
    border-collapse: collapse;
    margin-bottom: 25px;
    font-size: 10.5pt;
  }
  .info-table td {
    padding: 6px 12px;
    border: 1px solid #ccc;
    vertical-align: top;
  }
  .info-table td:first-child {
    font-weight: bold;
    width: 200px;
    background: #f7f7f7;
    color: #2c3e50;
  }

  /* Headings */
  h2 {
    font-size: 16pt;
    color: #2c3e50;
    border-bottom: 2px solid #2c3e50;
    padding-bottom: 4px;
    margin-top: 30px;
    margin-bottom: 15px;
    text-transform: uppercase;
    letter-spacing: 1px;
  }
  h3 {
    font-size: 13pt;
    color: #34495e;
    margin-top: 22px;
    margin-bottom: 10px;
    border-left: 4px solid #2c3e50;
    padding-left: 10px;
  }
  h4 {
    font-size: 11.5pt;
    color: #2c3e50;
    margin-top: 16px;
    margin-bottom: 8px;
    font-style: italic;
  }

  /* Paragraphs & Lists */
  p { margin-bottom: 10px; font-size: 10.5pt; }
  ul, ol { margin-left: 24px; margin-bottom: 12px; font-size: 10.5pt; }
  li { margin-bottom: 4px; }

  /* Tables */
  table {
    width: 100%;
    border-collapse: collapse;
    margin-bottom: 18px;
    font-size: 10pt;
  }
  th {
    background: #2c3e50;
    color: #fff;
    font-weight: bold;
    text-align: left;
    padding: 8px 10px;
    font-size: 10pt;
  }
  td {
    padding: 7px 10px;
    border: 1px solid #ddd;
    vertical-align: top;
  }
  tr:nth-child(even) { background: #f9f9f9; }

  /* Project Boxes */
  .project-box {
    border: 2px solid #2c3e50;
    border-radius: 6px;
    padding: 16px 20px;
    margin: 18px 0;
    background: #fafcff;
  }
  .project-box .project-title {
    font-size: 12pt;
    font-weight: bold;
    color: #2c3e50;
    margin-bottom: 8px;
    display: flex;
    align-items: center;
    gap: 8px;
  }
  .project-box .project-title .badge {
    background: #2c3e50;
    color: #fff;
    font-size: 8pt;
    padding: 2px 8px;
    border-radius: 3px;
    text-transform: uppercase;
    letter-spacing: 0.5px;
  }
  .project-box p, .project-box li { font-size: 10pt; }
  .project-box .due {
    font-weight: bold;
    color: #c0392b;
    margin-top: 8px;
    font-size: 10pt;
  }

  /* Semester Headers */
  .semester-header {
    background: #2c3e50;
    color: #fff;
    padding: 14px 20px;
    margin-top: 35px;
    margin-bottom: 20px;
    font-size: 15pt;
    font-weight: bold;
    letter-spacing: 1px;
    text-transform: uppercase;
  }
  .semester-sub {
    font-size: 10pt;
    font-weight: normal;
    display: block;
    margin-top: 4px;
    opacity: 0.85;
    letter-spacing: 0;
    text-transform: none;
  }

  /* Checklist */
  .checklist { list-style: none; margin-left: 0; }
  .checklist li {
    padding-left: 24px;
    position: relative;
    margin-bottom: 6px;
  }
  .checklist li::before {
    content: "\2610";
    position: absolute;
    left: 0;
    font-size: 13pt;
  }

  /* Architecture Diagram (text-based) */
  .arch-diagram {
    background: #f4f4f4;
    border: 1px solid #ccc;
    padding: 16px;
    font-family: 'Courier New', monospace;
    font-size: 9.5pt;
    line-height: 1.5;
    white-space: pre;
    overflow-x: auto;
    margin: 14px 0;
  }

  /* Footer */
  .footer {
    margin-top: 40px;
    padding-top: 12px;
    border-top: 1px solid #ccc;
    font-size: 9pt;
    color: #888;
    font-style: italic;
    text-align: center;
  }

  /* Priority table coloring */
  .keep-row { background: #e8f5e9 !important; }
  .compress-row { background: #fff8e1 !important; }
  .skip-row { background: #fce4ec !important; }
</style>
</head>
<body>

<!-- ===== TITLE BLOCK ===== -->
<div class="title-block">
  <h1>AI-DE 4900</h1>
  <div class="subtitle">AI Data Engineering &amp; Automation</div>
  <div class="program-line">Certificate-Style "Minor" Program &nbsp;|&nbsp; 12-Month Intensive</div>
</div>

<!-- ===== COURSE INFO ===== -->
<table class="info-table">
  <tr><td>Program Duration</td><td>12 Months (3 Semesters, 4 Months Each)</td></tr>
  <tr><td>Credit Hours</td><td>36 Credit Hours (12 per Semester)</td></tr>
  <tr><td>Delivery Format</td><td>Hybrid Online / Self-Directed Lab</td></tr>
  <tr><td>Office Hours</td><td>1 Session/Week (Peer Review &amp; Mistake Analysis)</td></tr>
  <tr><td>Prerequisites</td><td>Basic computer literacy; no prior programming required</td></tr>
  <tr><td>Required Materials</td><td>Coursera Plus subscription, GitHub account, local dev environment (Python 3.10+, VS Code, Docker, MySQL)</td></tr>
  <tr><td>Core Tech Stack</td><td>MySQL &bull; Python &bull; VS Code &bull; Docker &bull; Apache Airflow &bull; Git/GitHub</td></tr>
</table>

<!-- ===== PROGRAM DESCRIPTION ===== -->
<h2>Program Description</h2>
<p>This 12-month certificate program prepares students to build automated data pipelines, integrate Generative AI as a system component, and operate ML/LLM workflows reliably in production environments. Coursework combines structured online instruction with hands-on portfolio projects modeled after real-world AI Data Engineer, AI Platform Engineer, and Automation Engineer roles.</p>
<p>The program is built around a unified technology stack: <strong>MySQL</strong> for all relational database work, <strong>Python</strong> as the primary programming language, <strong>VS Code</strong> as the development environment, and <strong>Docker</strong> for containerized, reproducible infrastructure. This ensures every project runs identically across machines and mirrors modern production workflows.</p>
<p><strong>Program Outcome:</strong> <em>"I can build automated pipelines, add GenAI as a system component, and operate ML/LLM workflows reliably."</em></p>

<!-- ===== LEARNING OUTCOMES ===== -->
<h2>Program Learning Outcomes</h2>
<p>Upon successful completion of this program, students will be able to:</p>
<ol>
  <li>Write clean, tested Python scripts for data validation, transformation, and API integration</li>
  <li>Design and query MySQL databases for analytical and operational workloads</li>
  <li>Build and orchestrate automated ETL pipelines using Apache Airflow in Docker</li>
  <li>Containerize applications and data services using Docker and Docker Compose</li>
  <li>Integrate Large Language Models into data workflows as structured system components</li>
  <li>Version, evaluate, and monitor prompts and AI outputs in production settings</li>
  <li>Apply MLOps practices including CI/CD, drift monitoring, and runbook documentation</li>
</ol>

<!-- ===== INDUSTRY ALIGNMENT ===== -->
<h2>Industry Alignment</h2>
<p>This curriculum maps directly to requirements found in current AI Data Engineer and Automation postings:</p>
<table>
  <tr><th>Employer Requirement</th><th>Where Demonstrated</th></tr>
  <tr><td>Python + SQL + data integration</td><td>Projects 1&ndash;3, Historian, ETL Pipeline</td></tr>
  <tr><td>Scalable pipelines / reproducible workflows / CI/CD</td><td>Airflow Pipeline, Docker Compose, repo structure, automated tests</td></tr>
  <tr><td>Curated datasets + platform features</td><td>Design Doc, curated tables, MySQL data warehouse modeling</td></tr>
  <tr><td>GenAI workflows (LLMs, prompt assets)</td><td>AI Summarizer, LangChain apps, evaluation harness</td></tr>
  <tr><td>Industrial / IIoT flavor</td><td>Event &amp; time-series modeling, Diagnostics Historian</td></tr>
  <tr><td>Containerization &amp; DevOps</td><td>Docker throughout all projects, Docker Compose capstone</td></tr>
</table>
<p><em>Reference postings: AI Platform Data Engineer (Amazon/Ring), Senior Data Engineer (McKinsey/QuantumBlack), Data Engineer (Streamline Innovations/IIoT), Senior AI Platform Engineer (Guardian Life)</em></p>

<!-- ===== GRADING ===== -->
<h2>Grading &amp; Assessment</h2>
<table>
  <tr><th>Component</th><th>Weight</th></tr>
  <tr><td>Weekly Problem Sets &amp; Lab Assignments</td><td>20%</td></tr>
  <tr><td>Portfolio Projects (6 projects + 1 capstone)</td><td>45%</td></tr>
  <tr><td>Documentation Quality (READMEs, design docs, runbooks)</td><td>15%</td></tr>
  <tr><td>Monthly Self-Assessment &amp; Reflection</td><td>10%</td></tr>
  <tr><td>Final Capstone Presentation</td><td>10%</td></tr>
</table>

<h4>Grading Scale</h4>
<table>
  <tr><th>Grade</th><th>Percentage</th><th>Description</th></tr>
  <tr><td>A</td><td>90&ndash;100%</td><td>Exceptional; portfolio is interview-ready</td></tr>
  <tr><td>B</td><td>80&ndash;89%</td><td>Proficient; minor gaps in documentation or testing</td></tr>
  <tr><td>C</td><td>70&ndash;79%</td><td>Competent; projects functional but lack polish</td></tr>
  <tr><td>D</td><td>60&ndash;69%</td><td>Developing; significant gaps require remediation</td></tr>
  <tr><td>F</td><td>Below 60%</td><td>Incomplete; core projects missing or non-functional</td></tr>
</table>

<h4>Portfolio Project Rubric (Applied to Each Project)</h4>
<table>
  <tr><th>Criterion</th><th>Excellent (A)</th><th>Satisfactory (B&ndash;C)</th><th>Needs Work (D&ndash;F)</th></tr>
  <tr><td><strong>Functionality</strong></td><td>Runs end-to-end without errors</td><td>Runs with minor issues</td><td>Does not execute or major bugs</td></tr>
  <tr><td><strong>Code Quality</strong></td><td>Clean, modular, follows conventions</td><td>Readable but some disorganization</td><td>Difficult to follow, no structure</td></tr>
  <tr><td><strong>Testing</strong></td><td>Unit + integration tests, CI passing</td><td>Some tests present</td><td>No tests</td></tr>
  <tr><td><strong>Documentation</strong></td><td>Objective, Method, Results, Next Steps</td><td>README present but incomplete</td><td>No documentation</td></tr>
  <tr><td><strong>Version Control</strong></td><td>Feature branches, meaningful commits</td><td>Regular commits</td><td>Single commit or no history</td></tr>
</table>

<!-- ===== ACADEMIC POLICIES ===== -->
<h2>Academic Policies</h2>
<ul>
  <li><strong>Late Work:</strong> Assignments may be submitted up to 1 week late with a 10% penalty. Portfolio projects have firm monthly deadlines.</li>
  <li><strong>Academic Integrity:</strong> All code must be your own. AI assistants (ChatGPT, Copilot) may be used as learning tools but must be cited. Direct copy-paste without understanding is a violation.</li>
  <li><strong>Attendance:</strong> Weekly office hours are strongly recommended. Students who skip 3+ consecutive sessions must schedule a 1-on-1 check-in.</li>
  <li><strong>Accommodations:</strong> Flexible pacing is available. Students falling behind should communicate early.</li>
</ul>

<!-- ============================================================ -->
<!-- SEMESTER 1 -->
<!-- ============================================================ -->
<div class="page-break"></div>
<div class="semester-header">
  Semester 1: Foundations &amp; Data Handling
  <span class="semester-sub">Months 1&ndash;4 &nbsp;|&nbsp; 12 Credit Hours</span>
</div>
<p><strong>Semester Objective:</strong> Establish core programming, data manipulation, and software quality skills. Build first portfolio projects demonstrating Python and MySQL competency using VS Code and Docker.</p>

<!-- Unit 1 -->
<h3>Unit 1 &mdash; CS Fundamentals I: Programming &amp; Problem Solving</h3>
<p><strong>Month 1 &nbsp;|&nbsp; 3 Credit Hours</strong><br>
<strong>Course:</strong> <em>Programming with a Purpose</em> (Princeton University, via Coursera)</p>
<p>This unit establishes computational thinking and structured problem-solving. Students work through progressively challenging programming exercises, building fluency in variables, control flow, functions, arrays, and algorithmic reasoning. All work is done in VS Code from day one.</p>

<h4>Weekly Schedule</h4>
<table>
  <tr><th>Week</th><th>Topics</th><th>Assignments Due</th></tr>
  <tr><td>1</td><td>VS Code setup, Python environment, variables, types, operators, I/O</td><td>Problem Set 1 (3&ndash;4 problems)</td></tr>
  <tr><td>2</td><td>Conditionals, loops, basic debugging in VS Code</td><td>Problem Set 2 (3&ndash;4 problems)</td></tr>
  <tr><td>3</td><td>Functions, scope, recursion intro</td><td>Problem Set 3 (3&ndash;4 problems)</td></tr>
  <tr><td>4</td><td>Arrays, sorting basics, complexity intro</td><td>Problem Set 4 + Unit Reflection</td></tr>
</table>

<div class="project-box">
  <div class="project-title"><span class="badge">Micro-Project</span> cs-toolkit</div>
  <p>GitHub repository containing 8&ndash;12 solved problems with structured notes. Each solution must include: problem statement, approach narrative, time/space complexity analysis, and edge cases identified. All code written in VS Code and pushed via Git.</p>
  <p class="due">Due: End of Month 1</p>
</div>

<!-- Unit 2 -->
<h3>Unit 2 &mdash; Python for Data Engineering</h3>
<p><strong>Month 2 &nbsp;|&nbsp; 3 Credit Hours</strong><br>
<strong>Course:</strong> <em>Python and Pandas for Data Engineering</em> (Duke University, via Coursera)</p>
<p>Students transition from general programming to Python-specific data engineering skills using VS Code as the primary IDE. Emphasis on Pandas for data manipulation, file I/O, data parsing, API consumption, and building repeatable scripts for real-world data tasks.</p>

<h4>Weekly Schedule</h4>
<table>
  <tr><th>Week</th><th>Topics</th><th>Assignments Due</th></tr>
  <tr><td>5</td><td>Python data structures, Pandas DataFrames, VS Code workflow</td><td>Script 1: CSV file parser + cleaner with Pandas</td></tr>
  <tr><td>6</td><td>File I/O, string processing, error handling, linting in VS Code</td><td>Script 2: Log file analyzer</td></tr>
  <tr><td>7</td><td>HTTP requests, JSON parsing, API basics</td><td>Script 3: API data fetcher + JSON flattener</td></tr>
  <tr><td>8</td><td>Modules, packaging, script organization, virtual environments</td><td>Script 4: Multi-source data combiner + Project 1</td></tr>
</table>

<div class="project-box">
  <div class="project-title"><span class="badge">Project 1</span> Automation Data Validator</div>
  <p><strong>Objective:</strong> Build a production-style data validation tool for industrial device events.</p>
  <ul>
    <li>Reads CSV and JSON input files containing "device event" records</li>
    <li>Validates against a defined schema (required fields, data types)</li>
    <li>Validates value ranges (e.g., temperature within bounds, timestamp not in future)</li>
    <li>Logs all validation errors with row number, field, and reason</li>
    <li>Outputs a clean dataset (passing records only) + summary report (total records, pass/fail counts, error breakdown)</li>
  </ul>
  <p><strong>Deliverables:</strong> Working Python script/module (developed in VS Code), sample input data (100+ records, some malformed), sample output (clean dataset + error log + summary), README in lab report format (Objective, Method, Results, Next Steps).</p>
  <p class="due">Due: End of Month 2</p>
</div>

<!-- Unit 3 -->
<h3>Unit 3 &mdash; MySQL Foundations: Database Design &amp; Querying</h3>
<p><strong>Month 3 &nbsp;|&nbsp; 3 Credit Hours</strong><br>
<strong>Course:</strong> <em>Database Structures and Management with MySQL</em> (Meta, via Coursera)</p>
<p>Students learn relational database fundamentals using MySQL as the primary database engine. Covers schema design, CREATE TABLE, SELECT/WHERE/GROUP BY, JOINs, subqueries, stored procedures, and indexing. MySQL runs locally via Docker container, queried through VS Code with the MySQL extension.</p>

<h4>Weekly Schedule</h4>
<table>
  <tr><th>Week</th><th>Topics</th><th>Assignments Due</th></tr>
  <tr><td>9</td><td>MySQL in Docker setup, relational model, CREATE TABLE, INSERT, SELECT/WHERE</td><td>Query Drill Set 1 (8&ndash;10 queries)</td></tr>
  <tr><td>10</td><td>GROUP BY, HAVING, aggregate functions, ORDER BY, AUTO_INCREMENT</td><td>Query Drill Set 2 (8&ndash;10 queries)</td></tr>
  <tr><td>11</td><td>JOINs (INNER, LEFT, OUTER), subqueries, foreign keys</td><td>Query Drill Set 3 (8&ndash;10 queries)</td></tr>
  <tr><td>12</td><td>Stored procedures, indexes, query optimization basics</td><td>Query Drill Set 4 + SQL Exam + Project 2</td></tr>
</table>

<div class="project-box">
  <div class="project-title"><span class="badge">Project 2</span> Diagnostics Historian v1</div>
  <p><strong>Objective:</strong> Design and populate a MySQL database for industrial equipment diagnostics.</p>
  <ul>
    <li>Design MySQL schema with tables: <code>machines</code>, <code>devices</code>, <code>ports</code>, <code>events</code></li>
    <li>Establish proper foreign key relationships and indexing</li>
    <li>Load sample data (minimum 500 event records across 10+ devices)</li>
    <li>Write and document analytical queries: top 10 faults, downtime by device, worst ports, daily event trend, reporting gaps</li>
    <li>MySQL instance runs in Docker; Python script loads data via <code>mysql-connector-python</code></li>
  </ul>
  <p><strong>Deliverables:</strong> SQL schema file (DDL), Python data loading script, query file with all analytical queries + comments, Docker Compose file for MySQL, README with schema diagram and sample results.</p>
  <p class="due">Due: End of Month 3</p>
</div>

<!-- Unit 4 -->
<h3>Unit 4 &mdash; Docker, Git &amp; Software Quality</h3>
<p><strong>Month 4 &nbsp;|&nbsp; 3 Credit Hours</strong><br>
<strong>Courses:</strong> <em>Virtualization, Docker, and Kubernetes for Data Engineering</em> (Duke University, via Coursera) + Git Workflow Lab</p>
<p>This unit focuses on containerization and making existing code production-worthy. Students learn Docker fundamentals (Dockerfiles, images, Docker Compose), version control best practices, automated testing, and continuous integration &mdash; skills that transform "scripts" into "software."</p>

<h4>Weekly Schedule</h4>
<table>
  <tr><th>Week</th><th>Topics</th><th>Assignments Due</th></tr>
  <tr><td>13</td><td>Docker fundamentals: images, containers, Dockerfile, Docker CLI</td><td>Containerize Project 1 (Dockerfile + docker run)</td></tr>
  <tr><td>14</td><td>Docker Compose, multi-container apps, MySQL + Python in Docker</td><td>Create docker-compose.yml for Project 2 (MySQL + loader)</td></tr>
  <tr><td>15</td><td>Git fundamentals, feature branches, unit testing with pytest</td><td>Write tests for Project 1, add CI pipeline (GitHub Actions)</td></tr>
  <tr><td>16</td><td>Integration testing, Docker in CI, docs refresh</td><td>Full test suite for Project 2 + README upgrades</td></tr>
</table>

<div class="project-box">
  <div class="project-title"><span class="badge">Portfolio Upgrade</span> Containerization + Tests + CI</div>
  <p>Both Projects 1 and 2 must be updated with:</p>
  <ul>
    <li>Dockerfiles and/or Docker Compose configurations for reproducible execution</li>
    <li>Automated test suites (pytest) with minimum 70% code coverage</li>
    <li>CI pipeline (GitHub Actions) that builds Docker images and runs tests on every push</li>
    <li>READMEs reformatted to lab report style: <strong>Objective &rarr; Method &rarr; Results &rarr; Next Steps</strong></li>
  </ul>
  <p class="due">Due: End of Month 4</p>
</div>

<h4>Semester 1 Milestone Checklist</h4>
<ul class="checklist">
  <li><code>cs-toolkit</code> repository complete (8&ndash;12 problems, documented)</li>
  <li>Automation Data Validator &mdash; containerized, tested, CI passing, documented</li>
  <li>Diagnostics Historian v1 &mdash; MySQL in Docker, schema, data, queries, documented</li>
  <li>All repos use feature branches and meaningful commit messages</li>
  <li>Docker and Docker Compose used across all projects</li>
  <li>GitHub profile has pinned projects with clear READMEs</li>
</ul>

<!-- ============================================================ -->
<!-- SEMESTER 2 -->
<!-- ============================================================ -->
<div class="page-break"></div>
<div class="semester-header">
  Semester 2: Data Engineering &amp; Orchestration
  <span class="semester-sub">Months 5&ndash;8 &nbsp;|&nbsp; 12 Credit Hours</span>
</div>
<p><strong>Semester Objective:</strong> Build real data pipelines with orchestration, learn advanced MySQL data modeling, model industrial data, and polish everything to job-ready quality.</p>

<!-- Unit 5 -->
<h3>Unit 5 &mdash; ETL &amp; Orchestration: Automation Credibility</h3>
<p><strong>Month 5 &nbsp;|&nbsp; 3 Credit Hours</strong><br>
<strong>Course:</strong> <em>ETL and Data Pipelines with Shell, Airflow and Kafka</em> (IBM, via Coursera)</p>
<p>Students learn to build, schedule, and monitor automated data pipelines using Apache Airflow. The IBM course uses MySQL as the OLTP source database, aligning directly with our stack. Airflow runs via Docker Compose. This unit bridges the gap between "scripts that run" and "systems that operate."</p>

<h4>Weekly Schedule</h4>
<table>
  <tr><th>Week</th><th>Topics</th><th>Assignments Due</th></tr>
  <tr><td>17</td><td>ETL concepts, Shell scripting, Airflow architecture</td><td>Install Airflow (Docker Compose), build "Hello World" DAG</td></tr>
  <tr><td>18</td><td>DAG design, operators, task dependencies, scheduling</td><td>DAG 1: File ingestion + validation pipeline</td></tr>
  <tr><td>19</td><td>Error handling, retries, SLAs, alerting patterns</td><td>DAG 2: Multi-source ETL with retry logic</td></tr>
  <tr><td>20</td><td>XComs, templating, parameterization, monitoring</td><td>Portfolio Project 3</td></tr>
</table>

<div class="project-box">
  <div class="project-title"><span class="badge">Project 3</span> Automated ETL Pipeline (Airflow)</div>
  <p><strong>Objective:</strong> Build a fully orchestrated ETL pipeline that runs daily and produces auditable outputs.</p>
  <ul>
    <li><strong>Extract:</strong> Ingest raw data from file source (CSV/JSON, simulating upstream system)</li>
    <li><strong>Validate:</strong> Schema + range checks (reuse/extend Project 1 logic)</li>
    <li><strong>Transform:</strong> Clean, deduplicate, derive new fields (e.g., event severity classification)</li>
    <li><strong>Load:</strong> Insert into MySQL target tables via <code>mysql-connector-python</code></li>
    <li><strong>Report:</strong> Generate daily summary table + data quality report</li>
    <li><strong>Operational:</strong> Retry logic (3 attempts, exponential backoff), failure notifications, pipeline run log</li>
  </ul>
  <p><strong>Deliverables:</strong> Airflow DAG file(s), supporting Python modules, Docker Compose setup (Airflow + MySQL), sample pipeline run output, README with architecture diagram.</p>
  <p class="due">Due: End of Month 5</p>
</div>

<!-- Unit 6 -->
<h3>Unit 6 &mdash; Advanced Data Modeling &amp; MySQL Warehousing</h3>
<p><strong>Month 6 &nbsp;|&nbsp; 3 Credit Hours</strong><br>
<strong>Course:</strong> <em>Advanced Data Modeling</em> (Meta, via Coursera)</p>
<p>Students learn advanced MySQL data modeling using MySQL Workbench for visual design. Covers data warehousing concepts, dimensional modeling (star and snowflake schemas), ETL techniques for warehouse loading, and database optimization. This replaces cloud-specific content with hands-on MySQL architecture skills.</p>

<h4>Weekly Schedule</h4>
<table>
  <tr><th>Week</th><th>Topics</th><th>Assignments Due</th></tr>
  <tr><td>21</td><td>MySQL Workbench, ER diagrams, visual data modeling</td><td>Design Review 1: Reverse-engineer Historian schema in Workbench</td></tr>
  <tr><td>22</td><td>Data warehousing concepts, star schema, snowflake schema</td><td>Design Review 2: Warehouse schema for event analytics</td></tr>
  <tr><td>23</td><td>ETL for warehouse loading, slowly changing dimensions, partitioning</td><td>Build ETL: Load from OLTP tables into warehouse tables</td></tr>
  <tr><td>24</td><td>Query optimization, indexing strategies, EXPLAIN analysis</td><td>Portfolio Project 4</td></tr>
</table>

<div class="project-box">
  <div class="project-title"><span class="badge">Project 4</span> Industrial Data Platform Design Doc</div>
  <p><strong>Objective:</strong> Produce a professional architecture design document for an industrial data platform built on MySQL.</p>
  <ul>
    <li><strong>Architecture Diagram:</strong> Data flow from source through ingestion, processing, OLTP storage, and warehouse/serving layers</li>
    <li><strong>Component Descriptions:</strong> What each component does, why chosen, alternatives considered</li>
    <li><strong>Data Contracts:</strong> Schemas, SLAs, ownership for key data interfaces</li>
    <li><strong>OLTP vs. Warehouse Decision:</strong> Documented analysis of when to query OLTP vs. warehouse tables</li>
    <li><strong>Performance &amp; Scaling Notes:</strong> Indexing strategy, partitioning plan, query optimization approach</li>
    <li><strong>Security &amp; Access:</strong> MySQL user roles, privileges, encryption at rest/in transit</li>
  </ul>
  <p><strong>Deliverables:</strong> Design document (8&ndash;12 pages), MySQL Workbench ER diagrams, decision log (ADR format), README linking all artifacts.</p>
  <p class="due">Due: End of Month 6</p>
</div>

<!-- Unit 7 -->
<h3>Unit 7 &mdash; Automation Data Modeling (Industrial / IIoT)</h3>
<p><strong>Month 7 &nbsp;|&nbsp; 3 Credit Hours</strong><br>
<strong>Course:</strong> Project-Heavy Month (No new course &mdash; applied integration)</p>
<p>Students extend their MySQL Historian database and ETL pipeline to support industrial time-series data, event severity modeling, and operational metrics.</p>

<h4>Weekly Schedule</h4>
<table>
  <tr><th>Week</th><th>Topics</th><th>Assignments Due</th></tr>
  <tr><td>25</td><td>Time-series data modeling in MySQL, process values, sampling</td><td>Extend Historian schema: add time-series tables</td></tr>
  <tr><td>26</td><td>Event severity classification, alarm modeling</td><td>Implement severity taxonomy + enrich events</td></tr>
  <tr><td>27</td><td>Rolling windows, aggregations, anomaly flagging in MySQL</td><td>Build anomaly detection: spike-in-faults detector</td></tr>
  <tr><td>28</td><td>MTBF/MTTR proxy metrics, device metadata enrichment</td><td>Portfolio Project 5</td></tr>
</table>

<div class="project-box">
  <div class="project-title"><span class="badge">Project 5</span> Industrial Event Processor</div>
  <p><strong>Objective:</strong> Build an industrial data processing system that derives operational intelligence from raw device events.</p>
  <ul>
    <li><strong>Time-Series Support:</strong> Extend MySQL Historian schema with process value tables</li>
    <li><strong>Event Severity Model:</strong> Classify events into severity tiers with configurable rules</li>
    <li><strong>Device Metadata Enrichment:</strong> Join event data with device metadata</li>
    <li><strong>Anomaly Flagging:</strong> Detect spikes in fault frequency using rolling window analysis</li>
    <li><strong>Operational Metrics:</strong> MTBF proxy per device, rolling fault rate, "worst performers" ranking</li>
  </ul>
  <p><strong>Deliverables:</strong> Extended MySQL schema + migration scripts, Python processing modules, Docker Compose (MySQL + Python processor), sample outputs showing anomaly flags and metrics, README with data model diagram.</p>
  <p class="due">Due: End of Month 7</p>
</div>

<!-- Unit 8 -->
<h3>Unit 8 &mdash; Production Polish: Making It Job-Ready</h3>
<p><strong>Month 8 &nbsp;|&nbsp; 3 Credit Hours</strong><br>
<strong>Course:</strong> Applied Lab (No new course &mdash; portfolio refinement)</p>
<p>Students refactor, document, and present all work to professional standards. This unit teaches the meta-skill of "packaging yourself."</p>

<h4>Weekly Schedule</h4>
<table>
  <tr><th>Week</th><th>Topics</th><th>Assignments Due</th></tr>
  <tr><td>29</td><td>Repository hygiene: structure, naming, .gitignore, .dockerignore</td><td>Refactor all 5 project repos to standard structure</td></tr>
  <tr><td>30</td><td>Documentation: READMEs, inline comments, docstrings</td><td>Update all READMEs with screenshots + examples</td></tr>
  <tr><td>31</td><td>Portfolio site: structure, project pages, narrative</td><td>Build/update portfolio site Projects section</td></tr>
  <tr><td>32</td><td>Peer review simulation, presentation practice</td><td>Semester 2 Milestone Review</td></tr>
</table>

<h4>Semester 2 Milestone Checklist</h4>
<ul class="checklist">
  <li>Automated ETL Pipeline &mdash; Airflow + MySQL in Docker, tested, documented</li>
  <li>Industrial Data Platform Design Doc &mdash; MySQL Workbench diagrams, tradeoffs</li>
  <li>Industrial Event Processor &mdash; anomaly detection, metrics, documented</li>
  <li>All repos follow consistent structure and naming conventions</li>
  <li>Every project runs with a single <code>docker compose up</code> command</li>
  <li>Portfolio site live with project showcase pages</li>
  <li>GitHub profile polished: pinned repos, bio, contribution graph active</li>
</ul>

<!-- ============================================================ -->
<!-- SEMESTER 3 -->
<!-- ============================================================ -->
<div class="page-break"></div>
<div class="semester-header">
  Semester 3: GenAI + MLOps (AI Automation Specialization)
  <span class="semester-sub">Months 9&ndash;12 &nbsp;|&nbsp; 12 Credit Hours</span>
</div>
<p><strong>Semester Objective:</strong> Add AI/ML capabilities to existing data infrastructure. Learn to integrate, evaluate, version, and monitor GenAI components in production systems. Culminate in a capstone that ties the entire program together.</p>

<!-- Unit 9 -->
<h3>Unit 9 &mdash; AI Fundamentals: Business &amp; Systems View</h3>
<p><strong>Month 9 &nbsp;|&nbsp; 3 Credit Hours</strong><br>
<strong>Course:</strong> <em>AI for Everyone</em> (DeepLearning.AI, via Coursera)</p>
<p>Before building AI systems, students must understand what AI can and cannot do, where it fits in a data pipeline, and how to communicate about it with non-technical stakeholders.</p>

<h4>Weekly Schedule</h4>
<table>
  <tr><th>Week</th><th>Topics</th><th>Assignments Due</th></tr>
  <tr><td>33</td><td>What AI is / isn't, ML workflow overview, data's role</td><td>Reading Response: AI capabilities vs. hype</td></tr>
  <tr><td>34</td><td>AI project scoping, team structures, build vs. buy</td><td>Case Study: "Should this be AI or rules?" (3 scenarios)</td></tr>
  <tr><td>35</td><td>AI ethics, bias, failure modes, societal impact</td><td>Ethics Analysis: Identify failure modes in an AI system</td></tr>
  <tr><td>36</td><td>AI strategy, organizational readiness</td><td>Portfolio Addition: AI System Boundaries page</td></tr>
</table>

<div class="project-box">
  <div class="project-title"><span class="badge">Portfolio Addition</span> AI System Boundaries Page</div>
  <p>Add a page to your portfolio site demonstrating AI literacy and systems thinking:</p>
  <ul>
    <li><strong>What AI Does vs. What Rules Do:</strong> Framework for deciding when to use ML vs. deterministic logic</li>
    <li><strong>Failure Modes Taxonomy:</strong> Data drift, distribution shift, adversarial inputs, hallucination, feedback loops</li>
    <li><strong>Evaluation Approach:</strong> How to measure whether an AI component is working</li>
    <li><strong>Responsible AI Checklist:</strong> Personal framework for deploying AI ethically</li>
  </ul>
  <p class="due">Due: End of Month 9</p>
</div>

<!-- Unit 10 -->
<h3>Unit 10 &mdash; Generative AI Foundations</h3>
<p><strong>Month 10 &nbsp;|&nbsp; 3 Credit Hours</strong><br>
<strong>Course:</strong> <em>Generative AI with Large Language Models</em> (DeepLearning.AI &amp; AWS, via Coursera)</p>
<p>Students learn the architecture, training, and deployment of Large Language Models. The focus is practical: how to use LLMs as components in data systems, not how to train them from scratch.</p>

<h4>Weekly Schedule</h4>
<table>
  <tr><th>Week</th><th>Topics</th><th>Assignments Due</th></tr>
  <tr><td>37</td><td>Transformer architecture, pre-training, tokenization</td><td>Lab: Explore model behavior with different prompts</td></tr>
  <tr><td>38</td><td>Fine-tuning, PEFT/LoRA, instruction tuning</td><td>Lab: Compare base vs. instruction-tuned outputs</td></tr>
  <tr><td>39</td><td>RLHF, alignment, deployment considerations</td><td>Lab: Structured output generation (JSON mode)</td></tr>
  <tr><td>40</td><td>RAG, embeddings, LLM integration patterns</td><td>Portfolio Project 6</td></tr>
</table>

<div class="project-box">
  <div class="project-title"><span class="badge">Project 6</span> AI Diagnostics Summarizer</div>
  <p><strong>Objective:</strong> Build an AI-powered summarization system that converts raw diagnostic events into actionable intelligence.</p>
  <ul>
    <li><strong>Input:</strong> Query last 24 hours of events from the MySQL Diagnostics Historian</li>
    <li><strong>Processing:</strong> Send event data to an LLM with a structured prompt</li>
    <li><strong>Output:</strong> Structured JSON summary &mdash; severity assessment, top issues, likely root causes, recommended checks, confidence levels</li>
    <li><strong>Storage:</strong> Write summaries back to a <code>daily_summaries</code> table in the MySQL Historian</li>
    <li><strong>Evaluation:</strong> At least 5 test cases with expected vs. actual output comparison</li>
  </ul>
  <p><strong>Deliverables:</strong> Python module, versioned prompt template, test cases with evaluation results, Docker Compose (MySQL + summarizer), sample outputs, README with architecture and prompt design decisions.</p>
  <p class="due">Due: End of Month 10</p>
</div>

<!-- Unit 11 -->
<h3>Unit 11 &mdash; LLM Application Development &amp; Deployment</h3>
<p><strong>Month 11 &nbsp;|&nbsp; 3 Credit Hours</strong><br>
<strong>Course:</strong> <em>Building LLMs with Hugging Face and LangChain</em> (Edureka, via Coursera)</p>
<p>Students learn to build, containerize, and deploy LLM-powered applications using Python, LangChain, FastAPI, and Docker. This course replaces the browser-only prompt engineering approach with hands-on, code-first LLM development that aligns with our Docker + Python stack.</p>

<h4>Weekly Schedule</h4>
<table>
  <tr><th>Week</th><th>Topics</th><th>Assignments Due</th></tr>
  <tr><td>41</td><td>LangChain fundamentals, chains, prompt templates, output parsers</td><td>Rewrite Project 6 summarizer using LangChain; compare approaches</td></tr>
  <tr><td>42</td><td>RAG pipelines, document loaders, vector stores, retrieval chains</td><td>Build RAG pipeline over Historian documentation</td></tr>
  <tr><td>43</td><td>FastAPI for model serving, structured output contracts, evaluation</td><td>Build FastAPI service for the Summarizer with JSON schema validation</td></tr>
  <tr><td>44</td><td>Dockerize LLM apps, prompt versioning, regression testing</td><td>Portfolio Upgrade: Prompt Pack + Dockerized LLM Service</td></tr>
</table>

<div class="project-box">
  <div class="project-title"><span class="badge">Portfolio Upgrade</span> Prompt Pack + Dockerized LLM Service</div>
  <p><strong>Objective:</strong> Create a professional prompt engineering artifact and containerized LLM service.</p>
  <ul>
    <li><strong>Versioned Prompts:</strong> At least 3 versions with changelogs, managed via LangChain templates</li>
    <li><strong>Evaluation Rubric:</strong> Scoring criteria for accuracy, completeness, format compliance, actionability</li>
    <li><strong>Test Cases (10+):</strong> 5 golden inputs, 3 edge cases, 2 adversarial inputs</li>
    <li><strong>Prompt Contract:</strong> Input/output JSON schemas, constraints, failure behavior</li>
    <li><strong>Dockerized Service:</strong> FastAPI + LangChain app in Docker, reads from MySQL, returns structured JSON</li>
    <li><strong>Evaluation Results:</strong> Table showing each prompt version's score across test cases</li>
  </ul>
  <p class="due">Due: End of Month 11</p>
</div>

<!-- Unit 12 -->
<h3>Unit 12 &mdash; MLOps: Operate AI Reliably + Capstone</h3>
<p><strong>Month 12 &nbsp;|&nbsp; 3 Credit Hours</strong><br>
<strong>Course:</strong> <em>Machine Learning Engineering for Production (MLOps)</em> (DeepLearning.AI, via Coursera)</p>
<p>The final unit covers deployment, monitoring, versioning, and incident response for AI systems. The course includes Docker-based model serving labs. Students apply these concepts to their capstone project.</p>

<h4>Weekly Schedule</h4>
<table>
  <tr><th>Week</th><th>Topics</th><th>Assignments Due</th></tr>
  <tr><td>45</td><td>ML lifecycle, model serving with Docker, deployment strategies</td><td>Design: Deployment architecture for the Summarizer</td></tr>
  <tr><td>46</td><td>Data &amp; model monitoring, drift detection, alerting</td><td>Implement: Volume/distribution monitoring (MySQL queries)</td></tr>
  <tr><td>47</td><td>CI/CD/CT for ML, experiment tracking, reproducibility</td><td>Implement: Version tracking for prompts + data in MySQL</td></tr>
  <tr><td>48</td><td>Incident response, runbooks, post-mortems</td><td>Capstone final submission + presentation</td></tr>
</table>

<!-- CAPSTONE -->
<div class="page-break"></div>
<div class="semester-header">
  Capstone Project: AI Automation Data Platform
  <span class="semester-sub">Final Portfolio Centerpiece</span>
</div>

<p><strong>Objective:</strong> Integrate all program components into a single, end-to-end AI automation platform that demonstrates production-grade engineering practices. The entire platform runs via <code>docker compose up</code>.</p>

<h4>System Architecture</h4>
<div class="arch-diagram">
Raw Data (CSV/JSON/API)
      |
      v
[Airflow Pipeline] ---> Validate ---> Transform ---> Load
      |                    (Docker)                    |
      v                                                v
[Pipeline Logs]                              [MySQL Historian]
                                                       |
                                          +------------+------------+
                                          |                         |
                                          v                         v
                               [LangChain/FastAPI           [MySQL Warehouse
                                Summarizer Service]          Star Schema]
                                    (Docker)                        |
                                          |                         v
                                          v                [Analytics Queries]
                               [daily_summaries table]
                                          |
                                          v
                               [Monitoring &amp; Alerts]

All services containerized via Docker Compose:
  - mysql (database)
  - airflow-webserver + airflow-scheduler
  - summarizer-api (FastAPI + LangChain)
  - monitoring (drift detection cron)</div>

<h4>Required Components</h4>
<table>
  <tr><th>#</th><th>Component</th><th>Description</th></tr>
  <tr><td>1</td><td><strong>Airflow Pipeline</strong></td><td>Ingests, validates, transforms, loads data into MySQL on daily schedule. Retry logic, failure notifications, run logging. (From Project 3)</td></tr>
  <tr><td>2</td><td><strong>MySQL Historian</strong></td><td>Full schema: machines, devices, ports, events, time-series, severities. Analytical queries + operational metrics. Star schema warehouse tables. (From Projects 2, 4 &amp; 5)</td></tr>
  <tr><td>3</td><td><strong>AI Summarizer Service</strong></td><td>Dockerized FastAPI + LangChain service. Queries last 24h of events from MySQL. Generates structured JSON summaries via LLM. Stores back to database. (From Projects 6 &amp; 11)</td></tr>
  <tr><td>4</td><td><strong>Data Quality Tests</strong></td><td>Schema validation on ingestion, referential integrity checks, volume anomaly detection.</td></tr>
  <tr><td>5</td><td><strong>Prompt &amp; Version Tracking</strong></td><td>Prompt versions stored in MySQL and logged. Each summary linked to prompt version. Evaluation scores tracked.</td></tr>
  <tr><td>6</td><td><strong>Drift Monitoring</strong></td><td>Track fault volume over time. Alert on significant distribution shifts. Dashboard or trend report.</td></tr>
  <tr><td>7</td><td><strong>Runbook</strong></td><td>Documentation for pipeline failures, AI failures, and data quality failures. Each: symptoms, diagnosis, resolution, escalation.</td></tr>
</table>

<h4>Capstone Deliverables</h4>
<ul>
  <li>Complete GitHub repository with all components</li>
  <li>Single <code>docker-compose.yml</code> that spins up the entire platform (MySQL + Airflow + FastAPI Summarizer + monitoring)</li>
  <li>Architecture diagram (updated from Project 4, created in MySQL Workbench or draw.io)</li>
  <li>Runbook document</li>
  <li>5-minute presentation: problem statement, architecture walkthrough, live/recorded demo, key decisions, retrospective</li>
</ul>

<!-- ===== COURSE MAP ===== -->
<div class="page-break"></div>
<h2>Course-to-Project Mapping</h2>
<table>
  <tr><th>Course</th><th>Institution</th><th>Project</th></tr>
  <tr><td>Programming with a Purpose</td><td>Princeton</td><td><code>cs-toolkit</code> (solved problems repo)</td></tr>
  <tr><td>Python and Pandas for Data Engineering</td><td>Duke University</td><td>Project 1: Automation Data Validator</td></tr>
  <tr><td>Database Structures and Management with MySQL</td><td>Meta</td><td>Project 2: Diagnostics Historian v1</td></tr>
  <tr><td>Virtualization, Docker, and Kubernetes for DE</td><td>Duke University</td><td>Portfolio Upgrade: Containerization + CI</td></tr>
  <tr><td>ETL and Data Pipelines with Shell, Airflow and Kafka</td><td>IBM</td><td>Project 3: Automated ETL Pipeline</td></tr>
  <tr><td>Advanced Data Modeling</td><td>Meta</td><td>Project 4: Platform Design Doc</td></tr>
  <tr><td><em>(Project-heavy month)</em></td><td>&mdash;</td><td>Project 5: Industrial Event Processor</td></tr>
  <tr><td>AI for Everyone</td><td>DeepLearning.AI</td><td>AI System Boundaries (portfolio page)</td></tr>
  <tr><td>Generative AI with LLMs</td><td>DeepLearning.AI &amp; AWS</td><td>Project 6: AI Diagnostics Summarizer</td></tr>
  <tr><td>Building LLMs with Hugging Face and LangChain</td><td>Edureka</td><td>Prompt Pack + Dockerized LLM Service</td></tr>
  <tr><td>ML Engineering for Production (MLOps)</td><td>DeepLearning.AI</td><td>Capstone: AI Automation Data Platform</td></tr>
</table>

<!-- ===== TOOLS ===== -->
<h2>Recommended Tools &amp; Technologies</h2>
<table>
  <tr><th>Category</th><th>Tools</th></tr>
  <tr><td>Languages</td><td>Python 3.10+, SQL (MySQL dialect)</td></tr>
  <tr><td>IDE</td><td>VS Code with Python, MySQL, Docker, and Git extensions</td></tr>
  <tr><td>Version Control</td><td>Git, GitHub (Actions for CI/CD)</td></tr>
  <tr><td>Orchestration</td><td>Apache Airflow (via Docker Compose)</td></tr>
  <tr><td>Database</td><td>MySQL 8.0+ (local via Docker), MySQL Workbench</td></tr>
  <tr><td>AI / LLM</td><td>OpenAI API or Anthropic API, LangChain, Hugging Face, FastAPI</td></tr>
  <tr><td>Testing</td><td>pytest, Great Expectations (optional)</td></tr>
  <tr><td>Containers</td><td>Docker, Docker Compose</td></tr>
  <tr><td>Python Libraries</td><td>Pandas, mysql-connector-python, requests, FastAPI, uvicorn</td></tr>
  <tr><td>Documentation</td><td>Markdown, draw.io / Excalidraw, MySQL Workbench (ER diagrams)</td></tr>
</table>

<!-- ===== VS CODE EXTENSIONS ===== -->
<h4>Recommended VS Code Extensions</h4>
<table>
  <tr><th>Extension</th><th>Purpose</th></tr>
  <tr><td>Python (Microsoft)</td><td>Python language support, IntelliSense, debugging</td></tr>
  <tr><td>Pylance</td><td>Fast Python type checking and auto-complete</td></tr>
  <tr><td>MySQL (Weijan Chen)</td><td>Connect to MySQL in Docker, run queries, browse schemas</td></tr>
  <tr><td>Docker (Microsoft)</td><td>Manage containers, images, Compose files from VS Code</td></tr>
  <tr><td>Dev Containers</td><td>Develop inside Docker containers for consistent environments</td></tr>
  <tr><td>GitLens</td><td>Enhanced Git integration, blame, history</td></tr>
  <tr><td>Thunder Client</td><td>Test FastAPI endpoints without leaving VS Code</td></tr>
</table>

<!-- ===== PRIORITY GUIDANCE ===== -->
<h2>If Time Is Tight: Priority Guidance</h2>

<h4>Minimum Viable Path (Non-Negotiable)</h4>
<table>
  <tr><th>Priority</th><th>Course</th><th>Why</th></tr>
  <tr class="keep-row"><td>1</td><td>Python and Pandas for Data Engineering (Duke)</td><td>Foundation for everything; teaches VS Code workflow</td></tr>
  <tr class="keep-row"><td>2</td><td>Database Structures and Management with MySQL (Meta)</td><td>Required for every data role; MySQL-native</td></tr>
  <tr class="keep-row"><td>3</td><td>ETL and Data Pipelines (IBM/Airflow)</td><td>The core "data engineering" differentiator</td></tr>
  <tr class="keep-row"><td>4</td><td>Generative AI with LLMs (DeepLearning.AI)</td><td>The "AI" in "AI Data Engineer"</td></tr>
  <tr class="keep-row"><td>5</td><td>MLOps (DeepLearning.AI)</td><td>Production credibility; Docker for model serving</td></tr>
</table>

<h4>Compress First (Reduce but Don't Eliminate)</h4>
<table>
  <tr><th>Course</th><th>How to Compress</th></tr>
  <tr class="compress-row"><td>AI for Everyone</td><td>Complete in 1 weekend; keep notes for portfolio page</td></tr>
  <tr class="compress-row"><td>Advanced Data Modeling (Meta)</td><td>Focus on MySQL Workbench + warehouse schema; skip theory-heavy modules</td></tr>
</table>

<h4>Skip If Necessary (Lowest Risk to Cut)</h4>
<table>
  <tr><th>Course</th><th>When to Skip</th></tr>
  <tr class="skip-row"><td>Programming with a Purpose (Princeton)</td><td>Skip if you already have solid CS fundamentals</td></tr>
  <tr class="skip-row"><td>Virtualization, Docker, and Kubernetes (Duke)</td><td>Skip Kubernetes sections; focus on Docker + Docker Compose only</td></tr>
</table>

<!-- ===== COMPLETION ===== -->
<h2>Program Completion Requirements</h2>
<ol>
  <li><strong>Complete all 7 portfolio projects + capstone</strong> with a grade of C or above</li>
  <li><strong>Maintain a GitHub profile</strong> with all projects publicly accessible and documented</li>
  <li><strong>All projects must be containerized</strong> with Docker and runnable via <code>docker compose up</code></li>
  <li><strong>Deliver the capstone presentation</strong> (5 minutes, live or recorded)</li>
  <li><strong>Submit a program reflection</strong> (1&ndash;2 pages): key skills gained, biggest challenges, connection to target roles</li>
</ol>

<!-- ===== FINAL CHECKLIST ===== -->
<h2>Final Portfolio Checklist</h2>
<ul class="checklist">
  <li><code>cs-toolkit</code> &mdash; 8&ndash;12 solved problems with complexity analysis</li>
  <li>Automation Data Validator &mdash; Dockerized, tested, CI/CD, documented</li>
  <li>Diagnostics Historian v1 &mdash; MySQL in Docker, schema, data, analytical queries</li>
  <li>Automated ETL Pipeline &mdash; Airflow + MySQL in Docker, retry logic, quality reports</li>
  <li>Industrial Data Platform Design Doc &mdash; MySQL Workbench diagrams, warehouse schema, tradeoffs</li>
  <li>Industrial Event Processor &mdash; anomaly detection, MTBF metrics, Docker Compose</li>
  <li>AI System Boundaries &mdash; portfolio page on AI literacy</li>
  <li>AI Diagnostics Summarizer &mdash; LLM + MySQL integration, structured output</li>
  <li>Prompt Pack + Dockerized LLM Service &mdash; LangChain, FastAPI, versioned prompts, evaluation harness</li>
  <li><strong>Capstone: AI Automation Data Platform</strong> &mdash; full Docker Compose integration + runbook</li>
  <li>Portfolio site live with all project pages</li>
  <li>GitHub profile polished and professional</li>
</ul>

<!-- ===== FOOTER ===== -->
<div class="footer">
  <p>This syllabus is subject to adjustment based on student progress and emerging industry requirements.<br>
  The instructor reserves the right to modify assignments, deadlines, and course materials to better serve learning outcomes.<br>
  <strong>Core Stack:</strong> MySQL &bull; Python &bull; VS Code &bull; Docker</p>
</div>

</body>
</html>
